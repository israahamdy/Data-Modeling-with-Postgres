# Introduction
___
This is the fisrt project in the Udacity Data Engineering Nanodegree.
Its goal is to put the "Data Modeling with Postgres" course in a practical sense.
In this project, I'll apply what I've learned on data modeling with Postgres and build an ETL pipeline using Python.

# Project Summary
___
The purpose of this project is to help a startup called Sparkify that wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.
They'd like to create a Postgres database with tables designed to optimize queries on song play analysis. They want to create a database schema and ETL pipeline for this analysis. Also test the database and ETL pipeline by running queries given by the analytics team from Sparkify.

# Project Steps
___
- Create tables, define fact and dimension tables for a star schema for a particular analytic focus includes the following tables.
    > 1. Fact Table "songplays" with coulums (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

    > 2. Dimension Tables: 
        > 1. users - users in the app
        > (user_id, first_name, last_name, gender, level)
        > 2. songs - songs in music database
        > (song_id, title, artist_id, year, duration)
        > 3. artists - artists in music database
        > (artist_id, name, location, latitude, longitude)
        > 4. time - timestamps of records in songplays broken down into specific units
        > (start_time, hour, day, week, month, year, weekday)

![ER diagram](erd.png)

- Build ETL Processes and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

- Run Sanity Tests in the "test.ipynb" notebook to evaluate the code and catch any silly mistakes. test column data types, primary key constraints and not-null constraints as well look for on-conflict clauses wherever required.

# Project files
___
1. "data" folder: contains two folders for the datasets log_data and song_data
    > - Song Dataset: Each file is in JSON format and contains metadata about a song and the artist of that song.
        The files are partitioned by the first three letters of each song's track ID.

    > - Log Dataset: consists log files in JSON format generated by activity logs from a music streaming app based on specified configurations.

In addition to the data files, the project workspace includes five files:

2. test.ipynb : displays the first few rows of each table to let you check your database.
3. create_tables.py : drops and creates tables. this file to reset tables before run the ETL scripts.
4. etl.ipynb : reads and processes a single file from song_data and log_data and loads the data into tables. 
5. etl.py : reads and processes files from song_data and log_data and loads them into tables.
6. sql_queries.py : contains all sql queries, and is imported into the files above.

# How to run 
___
1. Run create_tables.py to set up the database and tables.
2. Run etl.py to process and load data into the database.
3. Launch test.ipynb to run validation and example queries.

# Author
___
Israa Hamdi [linkedin](https://www.linkedin.com/in/israa-hamdi/).